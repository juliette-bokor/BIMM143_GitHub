---
title: "Class 08 - Breast Cancer Mini Project"
author: "Juliette Bokor (PID: A16808121)"
format: pdf
---

## About 
In today's lab, we will work with fine needle aspiration (FNA) of breast mass data from the University of Wisconsin. 

##Data Import
```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

> Q. How many patients.individuals/samples are in this data set? 

```{r}
nrow(wisc.df)
```

> Q. How many of the observations have a malignant diagnosis? 

```{r}
table(wisc.df$diagnosis)
```

> Q. How many variables/features in the data are suffixed with _mean? 

```{r}
ncol(wisc.df)
colnames(wisc.df)
```

```{r}
inds <- grep("_mean", colnames(wisc.df), value =T)
inds
length(inds)
#used to find the number of terms
```


## Initial Analysis



## Clustering

Before analysis I want to make a new data set, removing the first column (diagnosis), which is essentially the answer. 
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
#using as.factor stores the data as a "factor", shows the levels at the bottom
diagnosis
```

```{r}
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

We can try a kmeans() clustering first
```{r}
km <- kmeans(wisc.data, centers=2)
#to find how many are in each cluster using table()
table(km$cluster)
```


Cross-table
```{r}
table(km$cluster, diagnosis)
#shows the number of malignant/benign in each cluster - this is not what we want; we want all the benign in one cluster, and all the malignant in the other cluster. 
```

Let's try `hclust()` the key input required is a distance matrix as produced by the `dist()` function. 

```{r}
hc <- hclust(dist(wisc.data))
```
I can make the dendrogram 
```{r}
plot(hc)
#this is not actually that useful, almost all the data is in a singular cluster, benign and malignant are not separated. 
```



## PCA

Do we need to scale the data? 

We can look at the sd of each column (oirginal variable)
```{r}
round(apply(wisc.data, 2, sd))
```

Yes we need to scale the data. We will run `prcomp()` with `scale=True`. 

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```


Generate our main PCA plot (score plot, PC1 vs PC2 plot).. 

```{r}
library(ggplot2)

res <- as.data.frame(wisc.pr$x)

#making a ggplot coloring by diagnosis, under aes instead of geom
ggplot(res) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

We see a visual separation of the two diagnoses based on a "line" when plotting PC1 vs PC2; it is separating out the cancer (malignant) from non-cancer (benign). 


## Combining Methods: Clustering on PCA results


Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
#doing it on PC1-PC3, not all the PCs (first three columns), need it as a distance vector
#covering 90% of the variability would require us to plot PC1 through PC7. 
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method="ward.D2")
plot(hc)

```


To get my clustering result/membership vector I need to "cut" the three with the `cutree()` function. 

```{r}
# you can use the argument k= to define the number of branches you want to cut the tree into 
grps <- cutree(hc, h=80)
```


> Q. How many patients are in each cluster group? 

```{r}
table(grps)
```

```{r}
plot(res$PC1, res$PC2, col=grps)
```


## Prediction 

We can use our PCA result (model) to do predictions, that is take new unseen data and project it onto our new PC variables. 

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


```{r}
plot(res$PC1, res$PC2, col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], labels=c(1,2), col="white")
```


## Summary

Principal Component Analysis (PCA) is a super useful method for analyzing large datasets. It works by finding new variables (PCs) that capture the most variance from the original variables in your dataset. 


> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

The hierarchical clustering of the PCA results has the best specificity and sensitivity of the analysis procedures that we ran. 

> Q18. Which of these new patients should we prioritize for follow up based on your results?

Patient 2 should be prioritized, they are in the cluster of the malignant data, indicating that they likely have a malignant cancer. 






